{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOoEU+qxNCTxamevZ/YGItv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chiyanglin-AStar/2025_physics_note/blob/main/02_Pytorch_Tutorial_I.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[PyTorch Tutorial - Learn PyTorch with Examples](https://www.geeksforgeeks.org/pytorch-learn-with-examples/)\n",
        "\n",
        "[pytorch_basics](https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/01-basics/pytorch_basics/main.py)"
      ],
      "metadata": {
        "id": "13XXPr52YNcr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##***PyTorch Basics:*** Tensors and GPU Acceleration in PyTorch\n",
        "###***1. Tensors in PyTorch***\n",
        "A ***tensor*** is a multi-dimensional array that is the fundamental data structure used in PyTorch (and many other machine learning frameworks). Tensors are crucial in PyTorch because they allow you to:\n",
        "\n",
        "***Store Data:*** Tensors can hold various types of data, including images, text, and numerical values.\n",
        "\n",
        "***Perform Calculations:*** You can carry out mathematical operations on tensors, which is essential for training machine learning models.\n",
        "\n",
        "***Utilize GPUs:*** Tensors can be moved to and processed on a Graphics Processing Unit (GPU), allowing for faster computations, especially important in deep learning tasks."
      ],
      "metadata": {
        "id": "iCeodyOLQJ-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Creating Tensor in PyTorch***\n",
        "\n",
        "We can create tensors for performing above in several ways:"
      ],
      "metadata": {
        "id": "n7dTIxm_RfEL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCt1doFTYGSl"
      },
      "outputs": [],
      "source": [
        "# Creating Tensors from Python Lists\n",
        "import torch\n",
        "my_tensor = torch.tensor([1, 2, 3])\n",
        "print(my_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Tensors from a NumPy array\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "numpy_array = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "torch_tensor = torch.from_numpy(numpy_array)\n",
        "print(\"\\nPyTorch Tensor:\")\n",
        "print(torch_tensor)"
      ],
      "metadata": {
        "id": "NcM3PPAeRwtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***2. Tensor Operations: Indexing, Slicing, Reshaping***\n",
        "\n",
        "PyTorch offers functionalities for tensor operations, including indexing, slicing, and reshaping. These operations are essential for manipulating data efficiently, especially when preparing data for machine learning tasks.\n",
        "\n",
        "***Indexing:***\n",
        "\n",
        "Indexing lets you retrieve specific elements or smaller sections from a larger tensor. For example, you can access a single number or a smaller block of numbers from the tensor.\n",
        "\n",
        "***Slicing:***\n",
        "\n",
        "Slicing allows you to take out a portion of the tensor by specifying a range of rows or columns. Itâ€™s like cutting a slice out of a cake, giving you just the part you want.\n",
        "\n",
        "***Reshaping:***\n",
        "\n",
        "Reshaping changes the shape or dimensions of a tensor without changing its actual data. This means you can reorganize the tensor into a different size while keeping all the original values intact."
      ],
      "metadata": {
        "id": "02mK9rZ8R244"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Create a 3x2 tensor\n",
        "tensor = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
        "\n",
        "# 1. Indexing: Access the element at row 1, column 0\n",
        "element = tensor[1, 0]\n",
        "print(f\"Indexed Element (Row 1, Column 0): {element}\")  # Outputs: 3\n",
        "\n",
        "# 2. Slicing: Extract the first two rows\n",
        "slice_tensor = tensor[:2, :]\n",
        "print(f\"Sliced Tensor (First two rows): \\n{slice_tensor}\")\n",
        "\n",
        "# 3. Reshaping: Reshape the tensor to a 2x3 tensor\n",
        "reshaped_tensor = tensor.view(2, 3)\n",
        "print(f\"Reshaped Tensor (2x3): \\n{reshaped_tensor}\")"
      ],
      "metadata": {
        "id": "Pj5_tgLoSWgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***3. Common Tensor Functions:***\n",
        "Broadcasting, Matrix Multiplication, etc.\n",
        "\n",
        "PyTorch offers a variety of common tensor functions that simplify complex operations. Such Common Tensors are:\n",
        "\n",
        "***Broadcasting***\n",
        "\n",
        "that allows for automatic expansion of dimensions to facilitate arithmetic operations on tensors of different shapes.\n",
        "\n",
        "***Matrix multiplication***\n",
        "\n",
        "it is also straightforward, that enables efficient computations essential for neural network operations."
      ],
      "metadata": {
        "id": "FK3ymk8tSeU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Create a 2x3 tensor\n",
        "tensor_a = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "\n",
        "# Create a 1x3 tensor (adjusted from 3x1)\n",
        "tensor_b = torch.tensor([[10, 20, 30]])  # Shape: (1, 3)\n",
        "\n",
        "# 1. Broadcasting: Add tensor_a and tensor_b\n",
        "broadcasted_result = tensor_a + tensor_b  # Now this will work\n",
        "print(f\"Broadcasted Addition Result: \\n{broadcasted_result}\")\n",
        "\n",
        "# 2. Matrix Multiplication: Multiply tensor_a and its transpose\n",
        "matrix_multiplication_result = torch.matmul(tensor_a, tensor_a.T)\n",
        "print(f\"Matrix Multiplication Result (tensor_a * tensor_a^T): \\n{matrix_multiplication_result}\")"
      ],
      "metadata": {
        "id": "8DqAk56oTAxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***4. GPU Acceleration with PyTorch***\n",
        "\n",
        "1.PyTorch facilitates GPU acceleration, enabling much faster computations, which is especially important in deep learning due to the extensive matrix operations involved. By transferring tensors to the GPU, you can significantly reduce training times and improve performance.\n",
        "\n",
        "2.PyTorch, it allows you to do this with minimal changes to your existing code, making it easy to take advantage of the speed of GPU processing."
      ],
      "metadata": {
        "id": "kbmd9uZDTKH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Step 1: Check if CUDA is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Step 2: Create a sample tensor and move it to the GPU\n",
        "tensor_size = (10000, 10000)  # Size of the tensor\n",
        "a = torch.randn(tensor_size, device=device)  # Random tensor on GPU\n",
        "b = torch.randn(tensor_size, device=device)  # Another random tensor on GPU\n",
        "\n",
        "# Step 3: Perform operations on GPU\n",
        "c = a + b  # Element-wise addition\n",
        "\n",
        "# Print the result (moving back to CPU for printing)\n",
        "print(\"Result shape (moved to CPU for printing):\", c.cpu().shape)\n",
        "\n",
        "# Optional: Check if GPU memory is being utilized\n",
        "print(\"Current GPU memory usage:\")\n",
        "print(f\"Allocated: {torch.cuda.memory_allocated(device) / (1024 ** 2):.2f} MB\")\n",
        "print(f\"Cached: {torch.cuda.memory_reserved(device) / (1024 ** 2):.2f} MB\")"
      ],
      "metadata": {
        "id": "4lFp-MDVTdma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##***Building and Training Neural Networks with PyTorch***\n",
        "\n",
        "###***Step 1: Define the Neural Network Class***"
      ],
      "metadata": {
        "id": "mFH5wn35TkY7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the Neural Network Class\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 4)  # Input layer to hidden layer\n",
        "        self.fc2 = nn.Linear(4, 1)   # Hidden layer to output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))  # Apply ReLU activation\n",
        "        x = self.fc2(x)               # Output layer\n",
        "        return x"
      ],
      "metadata": {
        "id": "l63d63zeT2Zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Step 2: Prepare the Data***"
      ],
      "metadata": {
        "id": "9Ma7vXBPUIYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the Data\n",
        "# Sample training data (features and labels)\n",
        "X_train = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])  # Inputs\n",
        "y_train = torch.tensor([[0.0], [1.0], [1.0], [0.0]])  # Corresponding outputs (XOR)"
      ],
      "metadata": {
        "id": "ZVIdd4Z3WBRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Step 3: Instantiate the Model, Loss Function, and Optimizer***"
      ],
      "metadata": {
        "id": "zDMc2oh2WF-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the Model, Define Loss Function and Optimizer\n",
        "model = SimpleNN()  # Instantiate the model\n",
        "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)  # Stochastic Gradient Descent"
      ],
      "metadata": {
        "id": "A4k4oatyWNRJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Step 5: Training the Model***\n",
        "\n",
        "Now we enter the training loop, where we will repeatedly pass our training data through the model to learn from it. Each iteration includes:\n",
        "\n",
        "1.Forward Pass: In a forward pass, the neural network processes input data step by step. Each layer applies a matrix multiplication (weights multiplied by the input) followed by an activation function to introduce non-linearity.\n",
        "\n",
        "2.Calculate Loss: Measure how far off the predictions are from the actual values.\n",
        "\n",
        "3.Backward Pass: Calculate the gradients of the loss with respect to the model parameters.\n",
        "\n",
        "4.Update Weights: Adjust the model parameters to minimize the loss.\n"
      ],
      "metadata": {
        "id": "-S6nF3MCWXSX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the Model\n",
        "for epoch in range(100):  # Run for 100 epochs\n",
        "    model.train()  # Set the model to training mode\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)  # Calculate the loss\n",
        "\n",
        "    # Backward pass and optimize\n",
        "    optimizer.zero_grad()  # Clear previous gradients\n",
        "    loss.backward()  # Compute gradients\n",
        "    optimizer.step()  # Update weights\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:  # Print loss every 10 epochs\n",
        "        print(f'Epoch [{epoch + 1}/100], Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "id": "fWPzsgvGWzia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Testing the Model"
      ],
      "metadata": {
        "id": "ciTGh-isW1ji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing the Model\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():  # Disable gradient calculation\n",
        "    test_data = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n",
        "    predictions = model(test_data)  # Get predictions\n",
        "    print(f'Predictions:\\n{predictions}')"
      ],
      "metadata": {
        "id": "IOAjEhDcW88b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete Code:"
      ],
      "metadata": {
        "id": "lzc5LwjAW-i5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Step 1: Define the Neural Network Class\n",
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(2, 4)  # Input layer to hidden layer\n",
        "        self.fc2 = nn.Linear(4, 1)   # Hidden layer to output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))  # Apply ReLU activation\n",
        "        x = self.fc2(x)               # Output layer\n",
        "        return x\n",
        "\n",
        "# Step 2: Prepare the Data\n",
        "# Sample training data (features and labels)\n",
        "X_train = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])  # Inputs\n",
        "y_train = torch.tensor([[0.0], [1.0], [1.0], [0.0]])  # Corresponding outputs (XOR)\n",
        "\n",
        "# Step 3: Instantiate the Model, Define Loss Function and Optimizer\n",
        "model = SimpleNN()  # Instantiate the model\n",
        "criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)  # Stochastic Gradient Descent with learning rate of 0.1\n",
        "\n",
        "# Step 4: Training the Model\n",
        "for epoch in range(100):  # Run for 100 epochs\n",
        "    model.train()  # Set the model to training mode\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)  # Calculate the loss\n",
        "\n",
        "    # Backward pass and optimize\n",
        "    optimizer.zero_grad()  # Clear previous gradients\n",
        "    loss.backward()  # Compute gradients\n",
        "    optimizer.step()  # Update weights\n",
        "\n",
        "    if (epoch + 1) % 10 == 0:  # Print loss every 10 epochs\n",
        "        print(f'Epoch [{epoch + 1}/100], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Step 5: Testing the Model\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "with torch.no_grad():  # Disable gradient calculation\n",
        "    test_data = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n",
        "    predictions = model(test_data)  # Get predictions\n",
        "    print(f'Predictions:\\n{predictions}')\n"
      ],
      "metadata": {
        "id": "XMAyP18aXCw2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}