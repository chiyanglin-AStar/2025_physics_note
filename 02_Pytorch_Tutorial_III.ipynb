{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyn4F9Do9DtYDU7so27+Nq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chiyanglin-AStar/2025_physics_note/blob/main/02_Pytorch_Tutorial_III.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[PyTorch Tutorial - Learn PyTorch with Examples](https://www.geeksforgeeks.org/pytorch-learn-with-examples/)"
      ],
      "metadata": {
        "id": "yOjr2Ou2ZQp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Tensors in PyTorch***\n",
        "\n",
        "A ***tensor*** is a multi-dimensional array that is the fundamental data structure used in PyTorch (and many other machine learning frameworks). Tensors are crucial in PyTorch because they allow you to:\n",
        "\n",
        "***Store Data:*** Tensors can hold various types of data, including images, text, and numerical values.\n",
        "\n",
        "***Perform Calculations:*** You can carry out mathematical operations on tensors, which is essential for training machine learning models.\n",
        "\n",
        "***Utilize GPUs:*** Tensors can be moved to and processed on a Graphics Processing Unit (GPU), allowing for faster computations, especially important in deep learning tasks."
      ],
      "metadata": {
        "id": "OXfZc4bSv1ni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***Creating Tensor in PyTorch***"
      ],
      "metadata": {
        "id": "VYWrpVHDwo2j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5kqeBZ8zZJmW"
      },
      "outputs": [],
      "source": [
        "# Creating Tensors from Python Lists\n",
        "import torch\n",
        "my_tensor = torch.tensor([1, 2, 3])\n",
        "print(my_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Tensors from a NumPy array\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "numpy_array = np.array([[1, 2, 3], [4, 5, 6]])\n",
        "torch_tensor = torch.from_numpy(numpy_array)\n",
        "print(\"\\nPyTorch Tensor:\")\n",
        "print(torch_tensor)"
      ],
      "metadata": {
        "id": "TJiNkX5qw6I5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***2. Tensor Operations: Indexing, Slicing, Reshaping***\n",
        "PyTorch offers functionalities for tensor operations, including indexing, slicing, and reshaping. These operations are essential for manipulating data efficiently, especially when preparing data for machine learning tasks.\n",
        "\n",
        "***Indexing:*** Indexing lets you retrieve specific elements or smaller sections from a larger tensor. For example, you can access a single number or a smaller block of numbers from the tensor.\n",
        "\n",
        "***Slicing:*** Slicing allows you to take out a portion of the tensor by specifying a range of rows or columns. Itâ€™s like cutting a slice out of a cake, giving you just the part you want.\n",
        "\n",
        "***Reshaping:*** Reshaping changes the shape or dimensions of a tensor without changing its actual data. This means you can reorganize the tensor into a different size while keeping all the original values intact."
      ],
      "metadata": {
        "id": "vGuQMz4QxBu5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Create a 3x2 tensor\n",
        "tensor = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
        "\n",
        "# 1. Indexing: Access the element at row 1, column 0\n",
        "element = tensor[1, 0]\n",
        "print(f\"Indexed Element (Row 1, Column 0): {element}\")  # Outputs: 3\n",
        "\n",
        "# 2. Slicing: Extract the first two rows\n",
        "slice_tensor = tensor[:2, :]\n",
        "print(f\"Sliced Tensor (First two rows): \\n{slice_tensor}\")\n",
        "\n",
        "# 3. Reshaping: Reshape the tensor to a 2x3 tensor\n",
        "reshaped_tensor = tensor.view(2, 3)\n",
        "print(f\"Reshaped Tensor (2x3): \\n{reshaped_tensor}\")"
      ],
      "metadata": {
        "id": "G1zurI-Lxbsi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***3. Common Tensor Functions: Broadcasting, Matrix Multiplication, etc.***\n",
        "\n",
        "PyTorch offers a variety of common tensor functions that simplify complex operations. Such Common Tensors are:\n",
        "\n",
        "Broadcasting that allows for automatic expansion of dimensions to facilitate arithmetic operations on tensors of different shapes.\n",
        "Matrix multiplication, it is also straightforward, that enables efficient computations essential for neural network operations."
      ],
      "metadata": {
        "id": "_8BCMwa6yGTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Create a 2x3 tensor\n",
        "tensor_a = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
        "\n",
        "# Create a 1x3 tensor (adjusted from 3x1)\n",
        "tensor_b = torch.tensor([[10, 20, 30]])  # Shape: (1, 3)\n",
        "\n",
        "# 1. Broadcasting: Add tensor_a and tensor_b\n",
        "broadcasted_result = tensor_a + tensor_b  # Now this will work\n",
        "print(f\"Broadcasted Addition Result: \\n{broadcasted_result}\")\n",
        "\n",
        "# 2. Matrix Multiplication: Multiply tensor_a and its transpose\n",
        "matrix_multiplication_result = torch.matmul(tensor_a, tensor_a.T)\n",
        "print(f\"Matrix Multiplication Result (tensor_a * tensor_a^T): \\n{matrix_multiplication_result}\")"
      ],
      "metadata": {
        "id": "o-sJqFvhzMZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###***4. GPU Acceleration with PyTorch***\n",
        "PyTorch facilitates GPU acceleration, enabling much faster computations, which is especially important in deep learning due to the extensive matrix operations involved. By transferring tensors to the GPU, you can significantly reduce training times and improve performance.\n",
        "PyTorch, it allows you to do this with minimal changes to your existing code, making it easy to take advantage of the speed of GPU processing."
      ],
      "metadata": {
        "id": "myxaX_7ozUaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Step 1: Check if CUDA is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "# Step 2: Create a sample tensor and move it to the GPU\n",
        "tensor_size = (10000, 10000)  # Size of the tensor\n",
        "a = torch.randn(tensor_size, device=device)  # Random tensor on GPU\n",
        "b = torch.randn(tensor_size, device=device)  # Another random tensor on GPU\n",
        "\n",
        "# Step 3: Perform operations on GPU\n",
        "c = a + b  # Element-wise addition\n",
        "\n",
        "# Print the result (moving back to CPU for printing)\n",
        "print(\"Result shape (moved to CPU for printing):\", c.cpu().shape)\n",
        "\n",
        "# Optional: Check if GPU memory is being utilized\n",
        "print(\"Current GPU memory usage:\")\n",
        "print(f\"Allocated: {torch.cuda.memory_allocated(device) / (1024 ** 2):.2f} MB\")\n",
        "print(f\"Cached: {torch.cuda.memory_reserved(device) / (1024 ** 2):.2f} MB\")"
      ],
      "metadata": {
        "id": "SEGDD2EGzZzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GY0K1_-_xavC"
      }
    }
  ]
}