{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOp+rPc1x7UITgUat9fQR7E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chiyanglin-AStar/2025_physics_note/blob/main/02_Pytorch_w_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch usage for GPU\n",
        "\n",
        "[How to use GPU acceleration in PyTorch?](https://www.geeksforgeeks.org/how-to-use-gpu-acceleration-in-pytorch/)\n",
        "\n",
        "[Use GPU in your PyTorch code](https://medium.com/ai%C2%B3-theory-practice-business/use-gpu-in-your-pytorch-code-676a67faed09)"
      ],
      "metadata": {
        "id": "y6WSF6Q5QPGI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***check GPU exist or not , method 1***"
      ],
      "metadata": {
        "id": "vn9SEV1xRsvP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCNDB8m0QNq6",
        "outputId": "2cdfe311-4a82-4295-929c-d89057f73226"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: Tesla T4 is available.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
        "else:\n",
        "    print(\"No GPU available. Training will run on CPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***check GPU exist or not , method 2***"
      ],
      "metadata": {
        "id": "-cA2JHXwZS-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "q7_CUxbKW45x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10a11039-eb73-4fa3-e442-55c2b4764713"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Moving Tensors to GPU***"
      ],
      "metadata": {
        "id": "D1tXH2LAY98-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Create a tensor on the CPU\n",
        "tensor = torch.randn((3, 3))\n",
        "#Move the tensor to the GPU\n",
        "tensor = tensor.to('cuda')"
      ],
      "metadata": {
        "id": "XgjMBH0nZf5E"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Parallel Processing with PyTorch***"
      ],
      "metadata": {
        "id": "sRewYuvwZmqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "model = nn.Conv2d(3, 64, 3, 1, 1) # Create a model\n",
        "\n",
        "model = nn.DataParallel(model) # Wrap the model with DataParallel\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model = model.to('cuda') # Move the model to the GPU\n",
        "  print(\"model data moved to GPU\")\n",
        "  # Perform forward pass on the model\n",
        "  input_data = torch.randn(20, 3, 32, 32).to('cuda')\n",
        "else:\n",
        "  print(\"No GPU available. Training will run on CPU.\")\n",
        "  # Perform forward pass on the model\n",
        "  input_data = torch.randn(20, 3, 32, 32)\n",
        "# Perform forward pass on the model\n",
        "#input_data = torch.randn(20, 3, 32, 32).to('cuda')\n",
        "output = model(input_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQEj4U5FZsMZ",
        "outputId": "6448237c-9aab-420f-f830-daa55e8eebfe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model data moved to GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Neural Network Training with GPU Acceleration***"
      ],
      "metadata": {
        "id": "7Ye5gjQFJ2gV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Example model\n",
        "class Generate(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generate, self).__init__()\n",
        "        self.gen = nn.Sequential(\n",
        "            nn.Linear(5,1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.gen(x)\n",
        "\n",
        "model = Generate() # Initialize the model\n",
        "if torch.cuda.is_available():\n",
        "  model.to('cuda') # Move the model to the GPU\n",
        "  print(\"model data moved to GPU\")\n",
        "  # Create input data inside GPU\n",
        "  input_data = torch.randn(16, 5, device='cuda')\n",
        "else:\n",
        "  print(\"No GPU available. Training will run on CPU.\")\n",
        "  # Create input data in CPU\n",
        "  input_data = torch.randn(16, 5)\n",
        "\n",
        "print(\"model data moved to GPU\")\n",
        "# Create input data inside GPU\n",
        "#input_data = torch.randn(16, 5, device=device) # original\n",
        "\n",
        "output = model(input_data) # Forward pass on theGP\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtUgdfL6JwKC",
        "outputId": "7a66c206-d154-4d86-899e-e9c1a3084e70"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model data moved to GPU\n",
            "model data moved to GPU\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4338],\n",
              "        [0.3197],\n",
              "        [0.4002],\n",
              "        [0.3765],\n",
              "        [0.2981],\n",
              "        [0.5240],\n",
              "        [0.3762],\n",
              "        [0.2910],\n",
              "        [0.3889],\n",
              "        [0.4951],\n",
              "        [0.6106],\n",
              "        [0.5551],\n",
              "        [0.4192],\n",
              "        [0.5376],\n",
              "        [0.4372],\n",
              "        [0.5015]], device='cuda:0', grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Pytorch , Cuda , Python , version***"
      ],
      "metadata": {
        "id": "x8PvVxs0jAiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import sys\n",
        "print('__Python VERSION:', sys.version)\n",
        "print('__pyTorch VERSION:', torch.__version__)\n",
        "print('__CUDA VERSION', )\n",
        "from subprocess import call\n",
        "# call([\"nvcc\", \"--version\"]) does not work\n",
        "! nvcc --version\n",
        "print('__CUDNN VERSION:', torch.backends.cudnn.version())\n",
        "print('__Number CUDA Devices:', torch.cuda.device_count())\n",
        "print('__Devices')\n",
        "# call([\"nvidia-smi\", \"--format=csv\", \"--query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\"])\n",
        "if torch.cuda.is_available():\n",
        "  print('Active CUDA Device: GPU', torch.cuda.current_device())\n",
        "print ('Available devices ', torch.cuda.device_count())\n",
        "if torch.cuda.is_available():\n",
        "  print ('Current cuda device ', torch.cuda.current_device())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5OsSHKfiqaI",
        "outputId": "f071b0f3-c9bc-4378-ca96-dbd66d4bd75d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "__Python VERSION: 3.11.11 (main, Dec  4 2024, 08:55:07) [GCC 11.4.0]\n",
            "__pyTorch VERSION: 2.5.1+cu121\n",
            "__CUDA VERSION\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n",
            "__CUDNN VERSION: 90100\n",
            "__Number CUDA Devices: 1\n",
            "__Devices\n",
            "Active CUDA Device: GPU 0\n",
            "Available devices  1\n",
            "Current cuda device  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***using PyCuda to get device name***"
      ],
      "metadata": {
        "id": "pmUcxVH9nxFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pycuda"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_RWINS9oNNi",
        "outputId": "89555809-0632-4e88-9c33-870691808604"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pycuda\n",
            "  Downloading pycuda-2024.1.2.tar.gz (1.7 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytools>=2011.2 (from pycuda)\n",
            "  Downloading pytools-2025.1.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from pycuda) (4.3.6)\n",
            "Collecting mako (from pycuda)\n",
            "  Downloading Mako-1.3.8-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from pytools>=2011.2->pycuda) (4.12.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from mako->pycuda) (3.0.2)\n",
            "Downloading pytools-2025.1.1-py3-none-any.whl (92 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.8/92.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Mako-1.3.8-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pycuda\n",
            "  Building wheel for pycuda (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycuda: filename=pycuda-2024.1.2-cp311-cp311-linux_x86_64.whl size=660362 sha256=3e3793e2376a5bd7c80bcb82ee272b849478753f6b6a23fbcf4bb3ccad298935\n",
            "  Stored in directory: /root/.cache/pip/wheels/76/66/50/c65e6116d7e0e16abe0f7c19b50327f76724ccfefbdc61a1b9\n",
            "Successfully built pycuda\n",
            "Installing collected packages: pytools, mako, pycuda\n",
            "Successfully installed mako-1.3.8 pycuda-2024.1.2 pytools-2025.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install pycuda\n",
        "import torch\n",
        "import pycuda.driver as cuda\n",
        "cuda.init()\n",
        "## Get Id of default device\n",
        "torch.cuda.current_device()\n",
        "# 0\n",
        "cuda.Device(0).name() # '0' is the id of your GPU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 42
        },
        "id": "Ilq0umTmnvaq",
        "outputId": "c1c0cc73-1cbc-4347-bba5-5967d266b166"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tesla T4'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    }
  ]
}