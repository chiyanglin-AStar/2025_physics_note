{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwE4u58FXL47YzUouvGy58",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chiyanglin-AStar/2025_physics_note/blob/main/02_Pytorch_w_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pytorch usage for GPU\n",
        "\n",
        "[How to use GPU acceleration in PyTorch?](https://www.geeksforgeeks.org/how-to-use-gpu-acceleration-in-pytorch/)\n",
        "\n",
        "[Use GPU in your PyTorch code](https://medium.com/ai%C2%B3-theory-practice-business/use-gpu-in-your-pytorch-code-676a67faed09)"
      ],
      "metadata": {
        "id": "y6WSF6Q5QPGI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***check GPU exist or not , method 1***"
      ],
      "metadata": {
        "id": "vn9SEV1xRsvP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCNDB8m0QNq6",
        "outputId": "2cdfe311-4a82-4295-929c-d89057f73226"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU: Tesla T4 is available.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)} is available.\")\n",
        "else:\n",
        "    print(\"No GPU available. Training will run on CPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***check GPU exist or not , method 2***"
      ],
      "metadata": {
        "id": "-cA2JHXwZS-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "q7_CUxbKW45x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10a11039-eb73-4fa3-e442-55c2b4764713"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Moving Tensors to GPU***"
      ],
      "metadata": {
        "id": "D1tXH2LAY98-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "# Create a tensor on the CPU\n",
        "tensor = torch.randn((3, 3))\n",
        "#Move the tensor to the GPU\n",
        "tensor = tensor.to('cuda')"
      ],
      "metadata": {
        "id": "XgjMBH0nZf5E"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Parallel Processing with PyTorch***"
      ],
      "metadata": {
        "id": "sRewYuvwZmqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "model = nn.Conv2d(3, 64, 3, 1, 1) # Create a model\n",
        "\n",
        "model = nn.DataParallel(model) # Wrap the model with DataParallel\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  model = model.to('cuda') # Move the model to the GPU\n",
        "  print(\"model data moved to GPU\")\n",
        "  # Perform forward pass on the model\n",
        "  input_data = torch.randn(20, 3, 32, 32).to('cuda')\n",
        "else:\n",
        "  print(\"No GPU available. Training will run on CPU.\")\n",
        "  # Perform forward pass on the model\n",
        "  input_data = torch.randn(20, 3, 32, 32)\n",
        "# Perform forward pass on the model\n",
        "#input_data = torch.randn(20, 3, 32, 32).to('cuda')\n",
        "output = model(input_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQEj4U5FZsMZ",
        "outputId": "6448237c-9aab-420f-f830-daa55e8eebfe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model data moved to GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***Neural Network Training with GPU Acceleration***"
      ],
      "metadata": {
        "id": "7Ye5gjQFJ2gV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Example model\n",
        "class Generate(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generate, self).__init__()\n",
        "        self.gen = nn.Sequential(\n",
        "            nn.Linear(5,1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.gen(x)\n",
        "\n",
        "model = Generate() # Initialize the model\n",
        "if torch.cuda.is_available():\n",
        "  model.to('cuda') # Move the model to the GPU\n",
        "  print(\"model data moved to GPU\")\n",
        "  # Create input data inside GPU\n",
        "  input_data = torch.randn(16, 5, device='cuda')\n",
        "else:\n",
        "  print(\"No GPU available. Training will run on CPU.\")\n",
        "  # Create input data in CPU\n",
        "  input_data = torch.randn(16, 5)\n",
        "\n",
        "print(\"model data moved to GPU\")\n",
        "# Create input data inside GPU\n",
        "#input_data = torch.randn(16, 5, device=device) # original\n",
        "\n",
        "output = model(input_data) # Forward pass on theGP\n",
        "output"
      ],
      "metadata": {
        "id": "EtUgdfL6JwKC",
        "outputId": "7a66c206-d154-4d86-899e-e9c1a3084e70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model data moved to GPU\n",
            "model data moved to GPU\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4338],\n",
              "        [0.3197],\n",
              "        [0.4002],\n",
              "        [0.3765],\n",
              "        [0.2981],\n",
              "        [0.5240],\n",
              "        [0.3762],\n",
              "        [0.2910],\n",
              "        [0.3889],\n",
              "        [0.4951],\n",
              "        [0.6106],\n",
              "        [0.5551],\n",
              "        [0.4192],\n",
              "        [0.5376],\n",
              "        [0.4372],\n",
              "        [0.5015]], device='cuda:0', grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}